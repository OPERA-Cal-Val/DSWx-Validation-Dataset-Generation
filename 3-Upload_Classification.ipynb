{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Staging Upload to ImageCalc Table\n",
    "This notebook goes through an example of uploading a classified planet image, a logfile, and training data to the DSWx calval database. Data files and metadata are uploading to a staging bucket. Then, the database manager will commit these uploads to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import os\n",
    "from datetime import datetime\n",
    "import sys\n",
    "sys.path.insert(0, './tools/')\n",
    "from addImageCalc import addImageCalc\n",
    "from pathlib import Path\n",
    "import rasterio\n",
    "from shapely.geometry import box\n",
    "from rasterio.crs import CRS\n",
    "from rasterio.warp import transform_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOU NEED TO FILL THESE OUT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name of person who reviewed the classification. Leave as \"None\" if classification has not been reviewed\n",
    "reviewer_name = '' \n",
    "\n",
    "#Usually 'Manual classification' or 'Review'\n",
    "calc_type = '' \n",
    "\n",
    "#Either 'Intermediate' or 'Final' if review is classification passed review with no changes\n",
    "processing_level = '' \n",
    "\n",
    "#Processing notes. e.g. 'Supervised classification using \n",
    "# SCP manual edits using Serval informed by Pekel water mask'\n",
    "notes = ''\n",
    "\n",
    "# Who made the classification - else specify \"review\"\n",
    "editor_name = '' \n",
    "\n",
    "fields = [editor_name, notes, processing_level, calc_type, reviewer_name]\n",
    "if any([not f for f in fields]):\n",
    "    raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only specify one. Leave the other as ''. If more than one planet image for given chip, PLANET_ID must be specified \n",
    "PLANET_ID = ''\n",
    "SITE_NAME = ''\n",
    "assert((len(PLANET_ID) == 0) ^ (len(SITE_NAME) == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Credentials\n",
    "In order to download imagery from the private bucket, JPL RSA access and OPERA Calval AWS credenitals are needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'opera-calval-database-dswx'\n",
    "session = boto3.session.Session(profile_name='saml-pub')\n",
    "s3 = session.resource('s3')\n",
    "s3_client = session.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Image metadata table\n",
    "To get the geometry metadata for the classified image. Make sure we connect `site_name` and `planet_id` correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageTable = gpd.read_file(s3.Object(bucket_name,'image.geojson').get()['Body'])\n",
    "imagecalcTable = gpd.read_file(s3.Object(bucket_name,'image_calc.geojson').get()['Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = imageTable[['image_name', 'site_name']]\n",
    "df_site2image = temp.set_index('site_name')\n",
    "df_image2site = temp.set_index('image_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will show the number of planet images found for a given chip. If more than one, ensure the printed Planet\n",
    "# ID matches the planet image used to generate the classification\n",
    "if not PLANET_ID:\n",
    "    values = PLANET_ID = df_site2image.loc[SITE_NAME].tolist()\n",
    "    PLANET_ID = values[0]\n",
    "    print(f'There was {len(values)} planet images for this chip')\n",
    "else:\n",
    "    values = df_image2site.loc[PLANET_ID].tolist()\n",
    "    SITE_NAME = values[0]\n",
    "    print(f'There were {len(values)} chips for this planet_image')\n",
    "\n",
    "(SITE_NAME, PLANET_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = imageTable[imageTable.image_name == PLANET_ID]\n",
    "planet_image = search.iloc[[0]]\n",
    "geometry = planet_image.geometry.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versioning simply increments to maximum version. If you downloaded imagery. Should have a metadata file from previous run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell assigns a version number to the classification. If this is the first classification of a given planet\n",
    "# image, the assigned version should be 0. Otherwise, it will increment on the latest version found in the database\n",
    "search = imagecalcTable[imagecalcTable.image_name == PLANET_ID]\n",
    "prev_version = -1\n",
    "if len(search) == 0:\n",
    "    previous_name = None\n",
    "    print('No previous versions found')\n",
    "else:\n",
    "    try:\n",
    "        prev_version = search['version'].max() \n",
    "        previous_name = search[search.version==search['version'].max()].image_calc_name.iloc[0]\n",
    "        print('Previous version = '+str(prev_version))\n",
    "    except:\n",
    "        previous_name = None\n",
    "        print('Older versions found, but could not identify version number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Previous Metadata or write new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagecalc_metadata_path = Path(f'planet_images_cropped/{PLANET_ID}/metadata_{PLANET_ID}_v{prev_version}.geojson')\n",
    "imagecalc_metadata_prev = {}\n",
    "if imagecalc_metadata_path.exists():\n",
    "    df_metadata = gpd.read_file(imagecalc_metadata_path)\n",
    "    imagecalc_metadata_prev = df_metadata.to_dict('records')[0]\n",
    "    \n",
    "imagecalc_metadata_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if imagecalc_metadata_path.exists():\n",
    "    editor_name =  imagecalc_metadata_prev['calculated_by']\n",
    "    print(f'Uploading the previous editor name ({editor_name})')\n",
    "    \n",
    "    notes = f'Previous({imagecalc_metadata_prev[\"notes\"]}) {notes}'\n",
    "    print(f'Notes: {notes}')\n",
    "    \n",
    "else:\n",
    "    if not editor_name:\n",
    "        raise ValueError('Update editor name in previous cell')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Local directory where classification file(s) are located\n",
    "uploadDir = Path(f'planet_images_cropped/{PLANET_ID}').absolute()\n",
    "\n",
    "#Name of classified geotif\n",
    "classified_image_filename = f'classification_finished_{PLANET_ID}.tif' \n",
    "\n",
    "#Uncomment if logfile exists and esnure it is included in filePaths below before upload\n",
    "#log_filename = 'logfile.txt \n",
    "\n",
    "#Uncomment if uploading any additional files and esnure they are included in filePaths below before upload\n",
    "#additional_filename = 'additional_file.txt' \n",
    "\n",
    "assert((uploadDir / classified_image_filename).exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-19T23:42:58.710258Z",
     "start_time": "2022-09-19T23:42:58.708000Z"
    }
   },
   "source": [
    "# Compute Stratum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(uploadDir / classified_image_filename) as ds:\n",
    "    water_mask = ds.read(1)\n",
    "    nodata = ds.nodata\n",
    "    classified_profile = ds.profile\n",
    "    bounds = ds.bounds\n",
    "    \n",
    "data_mask = (water_mask != nodata)\n",
    "water_frac = (water_mask == 1).sum() / data_mask.sum()\n",
    "print(f'Fraction of water is {water_frac: 1.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "bins = [0, .0008, .02, 1]\n",
    "def stratify(water_frac):\n",
    "    return np.digitize(water_frac, bins, right=True)\n",
    "water_stratum = stratify(water_frac)\n",
    "water_stratum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "print('Strata for percent water x')\n",
    "print(f'Strata 0: =={bins[0]}%')\n",
    "print(f'Strata 1: {bins[0] * 100} < x <= {bins[1] * 100}%')\n",
    "print(f'Strata 2: {bins[1] * 100} < x <= {bins[2] * 100}%')\n",
    "print(f'Strata 3: x > {bins[2] * 100}%')\n",
    "\n",
    "print()\n",
    "print('Example')\n",
    "for pw in [0, 0.0001, 0.0008, 0.001, 0.01, .03]:\n",
    "    ind = stratify(pw)\n",
    "    print(f'Area with {pw*100}% water is in strata {ind}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datatypes\n",
    "\n",
    "Let's make sure everything is in the right format.\n",
    "\n",
    "1. nodata = 255 (easier gdal reads)\n",
    "2. only values [0, 1, 7, 255] with\n",
    "\n",
    "+ 0 = no water\n",
    "+ 1 = water\n",
    "+ 10 = will not classify\n",
    "+ 255 = no data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can make this conditional flow `False` if you are annoyed by this additional check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    with rasterio.open(uploadDir / classified_image_filename) as ds:\n",
    "        water_mask = ds.read(1)\n",
    "        nodata = ds.nodata\n",
    "\n",
    "    assert(nodata == 255)\n",
    "\n",
    "    vals = set(np.unique(water_mask))\n",
    "    print(f'Vals: {vals}')\n",
    "    assert(vals.issubset(set([0, 1, 10, 255])))\n",
    "\n",
    "    assert(water_mask.dtype == 'uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should you want to either update nodata or dtype, change to True. Will update your data to append a `_formatted` suffix to the file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    classified_image_filename_old = classified_image_filename\n",
    "    classified_image_filename = classified_image_filename.replace('.tif', '_formatted.tif')\n",
    "    \n",
    "    p = classified_profile.copy()\n",
    "    p['nodata'] = 255\n",
    "    p['dtype'] = np.uint8\n",
    "    p['compress'] = 'lzw'\n",
    "    with rasterio.open(uploadDir / classified_image_filename, 'w', **p) as ds:\n",
    "        ds.write(water_mask.astype(np.uint8), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the polygon in 4326"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classified Image read and geometry\n",
    "\n",
    "with rasterio.open(uploadDir / classified_image_filename) as ds:\n",
    "    bounds = ds.bounds\n",
    "    crs_utm = ds.crs\n",
    "    \n",
    "bounds_4326 = transform_bounds(crs_utm, CRS.from_epsg(4326), *bounds)\n",
    "classified_geometry = box(*bounds_4326)\n",
    "classified_geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter the required file locations and metadata fields\n",
    "To upload the classified image, we need to specify its location on the local computer (and the location of auxilary files). We also need to fill in some metadata fields. Both file paths and metadata are specified as dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePaths = {\n",
    "    'image_calc' : str(uploadDir  / classified_image_filename),\n",
    "    #'logfile' : uploadDir + log_filename, #uncomment this line if uploading logfile\n",
    "    #additional_file: additional_file_name #uncomment this line if uploading additional file\n",
    "}\n",
    "filePaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaData = {\n",
    "    'image_name':planet_image.image_name.iloc[0], #str\n",
    "    'image_calc_name':planet_image.image_name.iloc[0]+'_classification', #str \n",
    "    'calc_type':calc_type, #str \n",
    "    'processing_level':processing_level, #str\n",
    "    'oversight_level':None, #str,\n",
    "    'calculated_by': editor_name, #str\n",
    "    'reviewed_by': reviewer_name, #str\n",
    "    'notes' : notes,\n",
    "    'public':True, #bool\n",
    "    'water_stratum': water_stratum,\n",
    "    'geometry': classified_geometry, #shapely geometry\n",
    "}\n",
    "metaData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage the image\n",
    "We use a pre-defined function to upload files and metadata to the staging area. This function takes the file paths and metadata dictionaries, as well as the AWS session object as inputs\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addImageCalc(filePaths,metaData,session)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
